pretrained_path: ./checkpoints/WariHima__voxcpm-1.5-resized-large
train_manifest: ./output/moriza-cv-courpus-ko/train.jsonl
val_manifest: ./output/moriza-cv-courpus-ko/val.jsonl
sample_rate: 44100
batch_size: 1
grad_accum_steps: 1  # Gradient accumulation steps, >1 can increase effective batch size without increasing memory
num_workers: 1
num_iters: 6000 #12000 #120000
log_interval: 10
valid_interval: 1000
save_interval: 1000
learning_rate: 0.0001
weight_decay: 0.01
warmup_steps: 100
max_steps: 6000
max_batch_tokens: 8192  # Example: single batch can have at most 16k tokens, with batch_size=4, each sample can have at most 4096 tokens
save_path: ./checkpoints/finetune_lora_ko
tensorboard: ./logs/finetune_lora_ko
lambdas:
  loss/diff: 1.0
  loss/stop: 1.0

# LoRA configuration
lora:
  enable_lm: true
  enable_dit: true
  enable_proj: false
  r: 32
  alpha: 16
  dropout: 0.0

# Distribution options (optional)
# - If distribute=false (default): save pretrained_path as base_model in lora_config.json
# - If distribute=true: save hf_model_id as base_model (hf_model_id is required)
# hf_model_id: "openbmb/VoxCPM1.5"
# distribute: true
