{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO0ZpWvaivy7"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "repo_name = \"VoxCPM-ja\"\n",
        "repo_url = \"https://github.com/q9uri/VoxCPM-ja.git\"\n",
        "import os\n",
        "\n",
        "if not os.path.exists(repo_name):\n",
        "    !git clone {repo_url}\n",
        "else:\n",
        "    print(f\"{repo_name} already exists. Pulling latest changes...\")\n",
        "\n",
        "%cd {repo_name}\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"pyproject.toml\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "text = text.replace('\"jpreprocess\",', \"\")\n",
        "\n",
        "with open(\"pyproject.toml\", \"w\") as f:\n",
        "    text = f.write(text)\n",
        "\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "DGpWqwv5Z0Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#patch g2p (colab and hf space don't install jpreprocess on 3.12)\n",
        "\n",
        "!pip install pyopenjtalk-plus\n",
        "\n",
        "text = \"\"\"\n",
        "import pyopenjtalk\n",
        "from .lib.ja.voicevox_g2p.kana_converter import create_kana\n",
        "from .lib.ja.voicevox_g2p.text_analyzer import full_context_labels_to_accent_phrases\n",
        "\n",
        "def g2p(norm_text) -> str:\n",
        "    full_context_labels = pyopenjtalk.extract_fullcontext(norm_text)\n",
        "    accent_phrases = full_context_labels_to_accent_phrases(full_context_labels)\n",
        "    kana = create_kana(accent_phrases)\n",
        "    return kana\n",
        "\"\"\"\n",
        "\n",
        "with open(\"src/voxcpm/text/japanese.py\", mode=\"w\") as f:\n",
        "  f.write(text)\n",
        "\n"
      ],
      "metadata": {
        "id": "doWY5AkChUpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download lora model\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "repo_id = \"WariHima/1.5-large-ja-rev-a\"\n",
        "filename = [\"lora_config.json\", \"lora_weights.safetensors\", \"optimizer.pth\", \"scheduler.pth\"]\n",
        "\n",
        "for file in filename:\n",
        "  path = hf_hub_download(\n",
        "      repo_id=repo_id,\n",
        "      filename=file,\n",
        "      local_dir=\"./checkpoints/1.5-large-ja-rev-a\",\n",
        "      local_dir_use_symlinks=False\n",
        "  )\n",
        "print(f\"Downloaded to: {path}\")"
      ],
      "metadata": {
        "id": "pYpqoNnVJPMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download asr model\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "repo_id = \"FunAudioLLM/SenseVoiceSmall\"\n",
        "filename = [\"am.mvn\", \"chn_jpn_yue_eng_ko_spectok.bpe.model\", \"config.yaml\", \"configuration.json\", \"model.pt\"]\n",
        "\n",
        "for file in filename:\n",
        "  path = hf_hub_download(\n",
        "      repo_id=repo_id,\n",
        "      filename=file,\n",
        "      local_dir=\"./checkpoints/SenseVoiceSmall\",\n",
        "      local_dir_use_symlinks=False\n",
        "  )\n",
        "print(f\"Downloaded to: {path}\")"
      ],
      "metadata": {
        "id": "NiDSbzN6MbgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"app.py\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# share=True の置換\n",
        "replace_list = [\n",
        "    \"interface.queue(max_size=10, default_concurrency_limit=1).launch(server_name=server_name, server_port=server_port, show_error=show_error)\",\n",
        "    \"interface.queue(max_size=10, default_concurrency_limit=1).launch(server_name=server_name, server_port=server_port, show_error=show_error, share=True)\",\n",
        "]\n",
        "text = text.replace(replace_list[0], replace_list[1])\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "#run gradio webui\n",
        "\n",
        "!python app.py"
      ],
      "metadata": {
        "id": "-vaIzqruNHYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#option\n",
        "#download korean lora model\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "repo_id = \"WariHima/1.5-large-ko-rev-a\"\n",
        "filename = [\"lora_config.json\",\n",
        "            \"lora_weights.safetensors\",\n",
        "            \"optimizer.pth\",\n",
        "            \"scheduler.pth\",\n",
        "            \"KoichiYasuoka__roberta-large-korean-morph-upos/config.json\",\n",
        "            \"KoichiYasuoka__roberta-large-korean-morph-upos/esupar.model\",\n",
        "            \"KoichiYasuoka__roberta-large-korean-morph-upos/pytorch_model.bin\",\n",
        "            \"KoichiYasuoka__roberta-large-korean-morph-upos/special_tokens_map.json\",\n",
        "            \"KoichiYasuoka__roberta-large-korean-morph-upos/tokenizer.json\",\n",
        "            \"KoichiYasuoka__roberta-large-korean-morph-upos/tokenizer_config.json\",\n",
        "            \"KoichiYasuoka__roberta-large-korean-morph-upos/vocab.txt\",\n",
        "            ]\n",
        "\n",
        "for file in filename:\n",
        "  path = hf_hub_download(\n",
        "      repo_id=repo_id,\n",
        "      filename=file,\n",
        "      local_dir=\"./checkpoints/1.5-large-ko-rev-a\",\n",
        "      local_dir_use_symlinks=False\n",
        "  )\n",
        "\n",
        "\n",
        "print(f\"Downloaded to: {path}\")"
      ],
      "metadata": {
        "id": "iov5pb1-O8Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# patch for colab in korean\n",
        "target_path = \"src/voxcpm/text/lib/ko/g2pk4/normalaizer/japanese.py\"\n",
        "with open(target_path, \"r\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "text = text.replace(\"from ....ja.voicevox_g2p import pyopenjtalk\", \"import pyopenjtalk\")\n",
        "\n",
        "with open(target_path, \"w\") as f:\n",
        "    f.write(text)\n"
      ],
      "metadata": {
        "id": "A_fmIpjRSyGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# korean language demo!\n",
        "# g2p周りのエラー出るけど動くよ！\n",
        "with open(\"app.py\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# share=True の置換\n",
        "replace_list = [\n",
        "    \"1.5-large-ja-rev-a\",\n",
        "    \"1.5-large-ko-rev-a\",\n",
        "]\n",
        "text = text.replace(replace_list[0], replace_list[1])\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "#run gradio webui\n",
        "\n",
        "!python app.py"
      ],
      "metadata": {
        "id": "M-J5Kfy4SxlG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}